{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be232b82",
   "metadata": {},
   "source": [
    "# Sparseness-Optimized Feature Importance (tabular)\n",
    "\n",
    "The algorithm finds a ranking of features $\\mathbf{\\Pi} = (\\pi_1 \\preceq \\pi_2 \\preceq, \\dots , \\preceq \\pi_n)$ such that $\\pi_i \\in \\mathbf{F}$ is the feature ranked in the $i$-th ranking position and $\\mathbf{F}$ is the set of all problem features. In the feature importance problem, it holds that $\\pi_i \\preceq \\pi_{i+1} \\iff g(\\pi_i) \\geq g(\\pi_{i+1})$ such that $g(\\pi_i)$ is a function that returns model's performance after maginalizing up to the $i$-th feature in the ranking.\n",
    "\n",
    "The proposed algorithm seeks to find $\\mathbf{\\Pi}$ using Genetic Algorithms in such a way that $g(\\pi_1) \\geq g(\\pi_2) \\geq \\dots \\geq g(\\pi_n)$. It aims to minimize the area under the curve given by the points $(1, g(\\pi_1)), (2, g(\\pi_2)), \\ldots, (n, g(\\pi_n))$ so that the curve is monotonically decreasing. The curve should not have peaks, although plateaus may occur. This behavior is enforces with a regularized penalization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed7721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.6, style='whitegrid')\n",
    "\n",
    "import pygad\n",
    "import shap\n",
    "import time\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a55a92",
   "metadata": {},
   "source": [
    "### Build and evaluate black-box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9681cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(dataset_path, test_size=0.2, random_state=42):\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Split the data into features (X) and the target variable (y)\n",
    "    X = data.drop(data.columns[-1], axis=1)\n",
    "    y = data[data.columns[-1]]\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_normalized = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(X_normalized, columns=X.columns)\n",
    "\n",
    "    # Encode target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Create a GradientBoostingClassifier model\n",
    "    model = GradientBoostingClassifier(random_state=random_state)\n",
    "\n",
    "    # Define hyperparameters for grid search\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.1, 0.01, 0.001]\n",
    "    }\n",
    "\n",
    "    # Perform grid search cross-validation\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='f1_macro')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Train the best model on the training data\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, best_model, f1_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77b576d",
   "metadata": {},
   "source": [
    "## SOFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d16d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_flip(model, X, y, permutation):\n",
    "    \n",
    "    scores = []\n",
    "    X_copy = X.copy()\n",
    "    \n",
    "    for feature in permutation:\n",
    "        \n",
    "        X_copy[X_copy.columns[feature]] = X_copy[X_copy.columns[feature]].mean()\n",
    "        y_pred = model.predict(X_copy)\n",
    "        f1 = f1_score(y, y_pred, average='macro')\n",
    "        scores.append(f1)\n",
    "        \n",
    "    return scores\n",
    "\n",
    "def fit_value(model, X, y, permutation):\n",
    "    \n",
    "    lambda_penalty = 0.1\n",
    "    weights = np.arange(1.0, 0.0, -1/(len(permutation)+1))\n",
    "    g_values = feature_flip(model, X, y, permutation)\n",
    "    g_values = [baseline] + g_values\n",
    "    \n",
    "    area_under_curve = np.sum(weights * g_values)\n",
    "    fitness = (area_under_curve + lambda_penalty * np.sum(np.maximum(np.diff(g_values), 0)))\n",
    "    \n",
    "    return fitness\n",
    "\n",
    "def fitness_func(ga_instance, solution, solution_idx):\n",
    "    \n",
    "    auc = fit_value(best_model, X_test, y_test, solution)\n",
    "    fitness = 1.0 / (auc + 0.000001) # for maximization\n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOFI_permutation(model, X, y):\n",
    "    ga_instance = pygad.GA(num_generations=20,\n",
    "                           num_parents_mating=20,\n",
    "                           fitness_func=fitness_func,\n",
    "                           sol_per_pop=100,\n",
    "                           num_genes=len(X.columns),\n",
    "                           init_range_low=0,\n",
    "                           init_range_high=len(X.columns)-1,\n",
    "                           parent_selection_type='sss',\n",
    "                           keep_parents=1,\n",
    "                           crossover_type='single_point',\n",
    "                           mutation_type='random',\n",
    "                           mutation_probability=0.01,\n",
    "                           gene_type=int,\n",
    "                           gene_space=range(0, len(X.columns)),\n",
    "                           allow_duplicate_genes=False,\n",
    "                           parallel_processing=8)\n",
    "\n",
    "    start_time = time.time()\n",
    "    ga_instance.run()\n",
    "    sofi_time = round(time.time() - start_time, 2)\n",
    "\n",
    "    # Returning the details of the best solution.\n",
    "    sofi_permutation, _, _ = ga_instance.best_solution(ga_instance.last_generation_fitness)\n",
    "\n",
    "    return sofi_permutation, sofi_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e34819",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ac8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SHAP_permutation(model, X_train, X_test):\n",
    "    start_time = time.time()\n",
    "    explainer = shap.KernelExplainer(model.predict_proba, shap.kmeans(X_train.values, 20))\n",
    "    shap_values = explainer.shap_values(X_test.values, silent=True)\n",
    "    shap_time = round(time.time() - start_time, 2)\n",
    "\n",
    "    # Calculate the average absolute SHAP values for each feature\n",
    "    avg_abs_shap_values = np.mean(np.abs(shap_values[0]), axis=0)\n",
    "\n",
    "    # Create a list of feature names\n",
    "    feature_names = range(X_train.shape[1])  # Replace with the actual feature names if available\n",
    "\n",
    "    # Create a dictionary to map feature names to their corresponding average SHAP values\n",
    "    feature_importance = dict(zip(feature_names, avg_abs_shap_values))\n",
    "\n",
    "    # Sort the features by their average SHAP values in descending order\n",
    "    sorted_feature_importance = {k: v for k, v in sorted(\n",
    "        feature_importance.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    shap_permutation = list(sorted_feature_importance.keys())\n",
    "\n",
    "    return shap_permutation, shap_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a9f55c",
   "metadata": {},
   "source": [
    "## PFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437fc1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PFI_permutation(model, X, y, n_repeats=50):\n",
    "    start_time = time.time()\n",
    "    output = permutation_importance(model, X, y, n_repeats=n_repeats, random_state=42)\n",
    "    pfi_time = round(time.time() - start_time, 2)\n",
    "\n",
    "    perm_importance = output.importances_mean\n",
    "\n",
    "    # Create a list of feature names\n",
    "    feature_names = range(X.shape[1])  # Replace with the actual feature names if available\n",
    "\n",
    "    # Create a dictionary to map feature names to their corresponding average permutation importances\n",
    "    pfi_feature_importance = dict(zip(feature_names, perm_importance))\n",
    "\n",
    "    # Sort the features by their average permutation importances in descending order\n",
    "    sorted_feature_importance = {k: v for k, v in sorted(\n",
    "        pfi_feature_importance.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    pfi_permutation = list(sorted_feature_importance.keys())\n",
    "\n",
    "    return pfi_permutation, pfi_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0cc285",
   "metadata": {},
   "source": [
    "### Plotting curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583df59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_flips(model, X, y, sofi_permutation, shap_permutation, pfi_permutation, baseline, dataset):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    random_permutation = list(sofi_permutation) # making a copy\n",
    "    random.shuffle(random_permutation)\n",
    "    g_values_Rand = feature_flip(model, X, y, random_permutation)\n",
    "    g_values_Rand = [baseline] + g_values_Rand\n",
    "\n",
    "    g_values_SOFI = feature_flip(model, X, y, sofi_permutation)\n",
    "    g_values_SOFI = [baseline] + g_values_SOFI\n",
    "\n",
    "    g_values_SHAP = feature_flip(model, X, y, shap_permutation)\n",
    "    g_values_SHAP = [baseline] + g_values_SHAP\n",
    "\n",
    "    g_values_PFI = feature_flip(model, X, y, pfi_permutation)\n",
    "    g_values_PFI = [baseline] + g_values_PFI\n",
    "\n",
    "    x_data = np.array(range(0, len(shap_permutation) + 1))\n",
    "    ax.plot(x_data, g_values_Rand, marker='', \n",
    "            markersize=10, linestyle='--', color='0.2', markerfacecolor='C3', label='Random')\n",
    "    ax.plot(x_data, g_values_SHAP, marker='o', \n",
    "            markersize=10, linestyle='-', color='0.2', markerfacecolor='C1', label='SHAP')\n",
    "    ax.plot(x_data, g_values_PFI, marker='o',\n",
    "            markersize=10, linestyle='-', color='0.2', markerfacecolor='C2', label='PFI')\n",
    "    ax.plot(x_data, g_values_SOFI, marker='o',\n",
    "            markersize=10, linestyle='-', color='0.2', markerfacecolor='C0', label='SOFI')\n",
    "\n",
    "    plt.gca().fill_between(x_data, g_values_SOFI, alpha=0.2, color='grey')\n",
    "    min_data = np.min([g_values_SOFI, g_values_SHAP, g_values_PFI, g_values_Rand], axis=0)\n",
    "    max_data = np.max([g_values_SOFI, g_values_SHAP, g_values_PFI, g_values_Rand], axis=0)\n",
    "    ax.set_ylim(min(min_data)-0.02, max(max_data)+0.03)\n",
    "\n",
    "    ax.set_xlabel('$\\pi_i$')\n",
    "    ax.set_ylabel('$g(\\pi_i)$')\n",
    "    ax.grid(False)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.savefig(dataset.replace('.csv', '.pdf'), format='pdf', bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d78ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc_values(model, X, y, baseline, sofi_permutation, shap_permutation, pfi_permutation):\n",
    "    \n",
    "    weights = np.arange(1.0, 0.0, -1/(len(sofi_permutation)+1)) \n",
    "    \n",
    "    g_values_SOFI = feature_flip(model, X, y, sofi_permutation)\n",
    "    g_values_SOFI = [baseline] + g_values_SOFI\n",
    "    area_under_curve = np.sum(weights * g_values_SOFI)\n",
    "    auc_SOFI = round(area_under_curve, 2)\n",
    "\n",
    "    g_values_SHAP = feature_flip(model, X, y, shap_permutation)\n",
    "    g_values_SHAP = [baseline] + g_values_SHAP \n",
    "    area_under_curve = np.sum(weights * g_values_SHAP)\n",
    "    auc_SHAP = round(area_under_curve, 2)\n",
    "\n",
    "    g_values_PFI = feature_flip(model, X, y, pfi_permutation)\n",
    "    g_values_PFI = [baseline] + g_values_PFI\n",
    "    area_under_curve = np.sum(weights * g_values_PFI)\n",
    "    auc_PFI = round(area_under_curve, 2)\n",
    "\n",
    "    return auc_SOFI, auc_SHAP, auc_PFI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1801973",
   "metadata": {},
   "source": [
    "### Experiment for several tabular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ee0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list CSV files in a directory\n",
    "def list_csv_files(directory):\n",
    "    return [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# Directory containing datasets\n",
    "datasets_folder = 'data'\n",
    "\n",
    "print(\"Dataset,SOFI_AUC,SHAP_AUC,PFI_AUC,SOFI_time,SHAP_time,PFI_time\")\n",
    "\n",
    "# Iterate over each dataset in the folder\n",
    "for dataset_file in list_csv_files(datasets_folder):\n",
    "    dataset_path = os.path.join(datasets_folder, dataset_file)\n",
    "\n",
    "    # Train and evaluate the model for the current dataset\n",
    "    X_train, X_test, y_train, y_test, best_model, baseline = train_and_evaluate_model(dataset_path)\n",
    "    \n",
    "    # Compute permutations for all feature importance methods\n",
    "    sofi_permutation, sofi_time = SOFI_permutation(best_model, X_test, y_test)\n",
    "    shap_permutation, shap_time = SHAP_permutation(best_model, X_train, X_test)\n",
    "    pfi_permutation, pfi_time = PFI_permutation(best_model, X_test, y_test)\n",
    "    \n",
    "    # Plot feature flipping curves\n",
    "    plot_feature_flips(best_model, X_test, y_test, sofi_permutation, shap_permutation, pfi_permutation, baseline, dataset_file)\n",
    "    \n",
    "    # Compute AUC for all permutations\n",
    "    auc_SOFI, auc_SHAP, auc_PFI = calculate_auc_values(best_model, X_test, y_test, baseline, sofi_permutation, shap_permutation, pfi_permutation)\n",
    "    \n",
    "    print(dataset_file.replace(\".csv\",\"\") + \",\"\n",
    "         + str(auc_SOFI) + \",\"\n",
    "         + str(auc_SHAP) + \",\"\n",
    "         + str(auc_PFI) + \",\"\n",
    "         + str(sofi_time) + \",\"\n",
    "         + str(shap_time) + \",\"\n",
    "         + str(pfi_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f55ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished signal \n",
    "import winsound\n",
    "winsound.Beep(1000, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
